{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shahe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmao = ['lmoa', 'bozen-bolzano']\n",
    "print(lmao.index('bozen-bolzano'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:36: SyntaxWarning: invalid escape sequence '\\R'\n",
      "<>:133: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:133: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:36: SyntaxWarning: invalid escape sequence '\\R'\n",
      "<>:133: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:133: SyntaxWarning: invalid escape sequence '\\|'\n",
      "C:\\Users\\shahe\\AppData\\Local\\Temp\\ipykernel_12580\\217881833.py:36: SyntaxWarning: invalid escape sequence '\\R'\n",
      "  docID = [int(c.rstrip('.txt')) for c in os.listdir(curr_dir + '\\ResearchPapers')] # extract the docIDs from the names of the files in the ResearchPapers directory\n",
      "C:\\Users\\shahe\\AppData\\Local\\Temp\\ipykernel_12580\\217881833.py:133: SyntaxWarning: invalid escape sequence '\\|'\n",
      "  tokens[j] = tokens[j].lstrip('0123456789!@#$%^&*()-_=+[{]}\\|;:\\'\",<.>/?`~').rstrip('0123456789!@#$%^&*()-_=+[{]}\\|;:\\'\",<.>/?`~').lower()\n",
      "C:\\Users\\shahe\\AppData\\Local\\Temp\\ipykernel_12580\\217881833.py:133: SyntaxWarning: invalid escape sequence '\\|'\n",
      "  tokens[j] = tokens[j].lstrip('0123456789!@#$%^&*()-_=+[{]}\\|;:\\'\",<.>/?`~').rstrip('0123456789!@#$%^&*()-_=+[{]}\\|;:\\'\",<.>/?`~').lower()\n"
     ]
    }
   ],
   "source": [
    "def get_stopwords():\n",
    "    \"\"\"\n",
    "    This function is used to extract stopwords from 'Stopword-List.txt' file.\n",
    "\n",
    "    It reads each line from the file, and if the line is not empty, it appends the line to the stopwords list.\n",
    "    The function continues this process until it reaches the end of the file. Assumes the file is in your current working directory.\n",
    "\n",
    "    Returns:\n",
    "        stopwords (list): A list of stopwords extracted from the file.\n",
    "    \"\"\"\n",
    "\n",
    "    stopwords = []\n",
    "    with open('Stopword-List.txt', 'r') as f: # The 'Stopword-List.txt' file is opened in read mode\n",
    "        while True:\n",
    "            text = f.readline() # Each line from the file is read one by one\n",
    "            if not text: # If the line read is empty (which means end of file), the loop is broken\n",
    "                break\n",
    "            stopwords.append(text) # else append the read line to the stopwords list\n",
    "\n",
    "    stopwords = [c.rstrip(' \\n') for c in stopwords if c != '\\n'] # A new list is created from stopwords, excluding any newline characters. Newline characters are also removed from the strings.\n",
    "    return stopwords\n",
    "\n",
    "def get_docIDs():\n",
    "    \"\"\"\n",
    "    This function is used to extract document IDs based on the names of the files in the 'ResearchPapers' directory.\n",
    "\n",
    "    It gets the current working directory and lists all the files in the 'ResearchPapers' directory. \n",
    "    It then extracts the document IDs from the names of these files, sorts them, and returns the sorted list.\n",
    "    Assumes the 'ResearchPapers' folder is in your current working directory.\n",
    "\n",
    "    Returns:\n",
    "        docID (list): A sorted list of document IDs extracted from the file names in the 'ResearchPapers' directory.\n",
    "    \"\"\"\n",
    "\n",
    "    curr_dir = os.getcwd() # get the current directory\n",
    "    docID = [int(c.rstrip('.txt')) for c in os.listdir(curr_dir + '\\ResearchPapers')] # extract the docIDs from the names of the files in the ResearchPapers directory\n",
    "    docID.sort()\n",
    "    return docID\n",
    "\n",
    "def create_positional_index(total_tokens):\n",
    "    \"\"\"\n",
    "    This function creates a positional index from the given tokens.\n",
    "\n",
    "    Args:\n",
    "        total_tokens (list): A list of processed tokens from which the positional index is to be created.\n",
    "\n",
    "    Returns:\n",
    "        terms (dict): A dictionary representing the positional index.\n",
    "    \"\"\"\n",
    "\n",
    "    terms = {} # declare an empty dictionary for the positional index\n",
    "    porter_stemmer = PorterStemmer() # initialize the stemmer\n",
    "\n",
    "    # get the stopwords. Although stopwords is not going to be inserted in the positional index, we still need them to find the correct positions of the rest of the words\n",
    "    stopwords = get_stopwords()\n",
    "    doc = get_docIDs() # get the docIDs\n",
    "\n",
    "    for i, tokens in enumerate(total_tokens): # Loop through each token in total_tokens\n",
    "        for j, word in enumerate(tokens): # Loop through each word in tokens\n",
    "            word = porter_stemmer.stem(word) # Stem the word\n",
    "            if word[-1] == \"'\": # If the word ends with an apostrophe, remove it\n",
    "                word = word.rstrip(\"'\")\n",
    "            if word not in stopwords: # Filter the stopwords\n",
    "                if word in terms: # If the word is already in the positional index, add the docID to the index\n",
    "                    if (i+1) in terms[word]: # If the docID is already in the index for that word, add the position\n",
    "                        terms[word][i+1].append(j+1)\n",
    "                    else: # else add the docID as well as the position\n",
    "                        terms[word][i+1] = [j+1]\n",
    "                else: # Add the word in the index along with the docID and the position\n",
    "                    terms[word] = {i+1: [j+1]}\n",
    "\n",
    "    return terms\n",
    "\n",
    "def create_inverted_index(total_tokens):\n",
    "    \"\"\"\n",
    "    This function creates an inverted index from the given tokens.\n",
    "\n",
    "    Args:\n",
    "        total_tokens (list): A list of tokens from which the inverted index is to be created.\n",
    "\n",
    "    Returns:\n",
    "        terms (dict): A dictionary representing the inverted index.\n",
    "    \"\"\"\n",
    "\n",
    "    terms = {} # An empty dictionary for the inverted index\n",
    "    porter_stemmer = PorterStemmer() # initialize the stemmer\n",
    "    stopwords = get_stopwords() # get the stopwords\n",
    "    doc = get_docIDs() # get the docIDs\n",
    "\n",
    "    for i, tokens in enumerate(total_tokens): # Loop through each token in total_tokens and remove the stopwords\n",
    "        total_tokens[i] = [c for c in tokens if c not in stopwords]\n",
    "\n",
    "    for i, tokens in enumerate(total_tokens): # Loop through each token in total_tokens again\n",
    "        for word in tokens: # Loop through each word in tokens\n",
    "            word = porter_stemmer.stem(word) # Stem the word\n",
    "            if word[-1] == \"'\": # remove the apostrophe\n",
    "                word = word.rstrip(\"'\")\n",
    "            if word in terms: # If the word is already in the inverted index\n",
    "                if (i+1) not in terms[word]: # Append the docID if it isn't in the index\n",
    "                    terms[word].append(i+1)\n",
    "            else: # Add the word among with the docID\n",
    "                terms[word] = [i+1]\n",
    "\n",
    "    return terms    \n",
    "\n",
    "def preprocessing():\n",
    "\n",
    "    \"\"\"\n",
    "    This function is used to preprocess the text files in the 'ResearchPapers' directory.\n",
    "\n",
    "    It reads each file, tokenizes the text, removes punctuation and converts the text to lowercase. \n",
    "    It also splits the tokens at '.' and '-'. Assumes the 'ResearchPapers' folder is in your current working directory.\n",
    "\n",
    "    Returns:\n",
    "        total_tokens (list): A list of preprocessed tokens from all the files.\n",
    "    \"\"\"\n",
    "\n",
    "    total_tokens = [] # An empty list to store the tokens from all the files\n",
    "    doc = get_docIDs() # get the docIDs\n",
    "\n",
    "    for i in doc: # iterate through each doc\n",
    "        tokens = []\n",
    "        with open('ResearchPapers/' + str(i) +'.txt', 'r') as f: # open the file corresponding to the current document ID\n",
    "            while True:\n",
    "                text = f.readline() # read a line from the file\n",
    "                if not text: # if the line is empty (which means end of file), break the loop\n",
    "                    break\n",
    "                tokens += word_tokenize(text) # tokenize the line and add the tokens to the list\n",
    "        \n",
    "        j = 0\n",
    "        while j < len(tokens): # Loop through each token\n",
    "            # Remove symbols and numbers from the start and end of the token and convert it to lowercase (case folding)\n",
    "            tokens[j] = tokens[j].lstrip('0123456789!@#$%^&*()-_=+[{]}\\|;:\\'\",<.>/?`~').rstrip('0123456789!@#$%^&*()-_=+[{]}\\|;:\\'\",<.>/?`~').lower()\n",
    "            if '.' in tokens[j]: # if '.' exists in a word, split the word at that point and add the splitted words at the end of the tokens list while removing the original word\n",
    "                word = tokens[j].split('.')\n",
    "                del tokens[j]\n",
    "                tokens.extend(word)\n",
    "            elif '-' in tokens[j]: # do the same for words with '-'\n",
    "                word = tokens[j].split('-')\n",
    "                del tokens[j]\n",
    "                tokens.extend(word)\n",
    "            j += 1 # move the index forward\n",
    "        tokens = [c for c in tokens if c.isalpha()] # filter out any strings that contain symbols, numbers, etc.\n",
    "        total_tokens.append(tokens) # add the processed tokens as a seperate list. Did this to keep track of which tokens appear in which docs (needed to construct indexes). List at index 0 indicate tokens found in doc 1 and so on.\n",
    "    return total_tokens\n",
    "\n",
    "def save_indexes():\n",
    "    tokens = preprocessing()\n",
    "    inverted_index = create_inverted_index(tokens)\n",
    "    print(len(inverted_index))\n",
    "    positional_index = create_positional_index(tokens)\n",
    "    print(len(positional_index))\n",
    "\n",
    "    with open('inverted_index.txt', 'w') as f:\n",
    "        for key, value in inverted_index.items():\n",
    "            f.write('{}:{}\\n'.format(key, value))\n",
    "    with open('positional_index.txt', 'w') as f:\n",
    "        for key, value in positional_index.items():\n",
    "            f.write('{}:{}\\n'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16278\n",
      "16263\n"
     ]
    }
   ],
   "source": [
    "save_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inverted_index[PorterStemmer().stem('feature')])\n",
    "print(inverted_index[PorterStemmer().stem('selection')])\n",
    "print(inverted_index[PorterStemmer().stem('redundency')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inverted_index[PorterStemmer().stem('classification')])\n",
    "print(inverted_index[PorterStemmer().stem('selection')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inverted_index[PorterStemmer().stem('heart')])\n",
    "print(inverted_index[PorterStemmer().stem('attack')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'cancer AND learning'\n",
    "processed_query = query.split()\n",
    "print(processed_query)\n",
    "\n",
    "for i in processed_query:\n",
    "    if i == 'AND' or i == 'OR' or i == 'NOT':\n",
    "        condition.append(i)\n",
    "    else:\n",
    "        terms.append(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
